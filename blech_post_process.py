import os
import tables
import numpy as np
import easygui
import re
import pylab as plt
import matplotlib.image as mpimg
from sklearn.mixture import GaussianMixture
import argparse
import pandas as pd

# Import 3rd party code
from utils import blech_waveforms_datashader
from utils.blech_utils import entry_checker, imp_metadata

# Set seed to allow inter-run reliability
# Also allows reusing the same sorting sheets across runs
np.random.seed(0)

def cluster_check(x):
	clusters = re.findall('[0-9]+',x)
	return sum([i.isdigit() for i in clusters]) == len(clusters)

# Get directory where the hdf5 file sits, and change to that directory
# Get name of directory with the data files
# Create argument parser
parser = argparse.ArgumentParser(description = 'Spike extraction and sorting script')
parser.add_argument('--dir-name',  '-d', help = 'Directory containing data files')
parser.add_argument('--taste',  '-t', help = 'True or False for Taste Interval Processed')
parser.add_argument('--show-plot', '-p', 
		help = 'Show waveforms while iterating (True/False)', default = 'True')
parser.add_argument('--sort-file', '-f', help = 'CSV with sorted units')
args = parser.parse_args()

if args.sort_file is not None:
	if not (args.sort_file[-3:] == 'csv'):
		raise Exception("Please provide CSV file")
	sort_table = pd.read_csv(args.sort_file)
	sort_table.fillna('',inplace=True)
	# Check when more than one cluster is specified
	sort_table['len_cluster'] = \
			[len(re.findall('[0-9]+',str(x))) for x in sort_table.Cluster]
	# Get splits and merges out of the way first
	sort_table.sort_values(['len_cluster','Split'],ascending=False, inplace=True)
	true_index = sort_table.index
	sort_table.reset_index(inplace=True)

if args.dir_name is not None: 
	metadata_handler = imp_metadata([[],args.dir_name])
else:
	metadata_handler = imp_metadata([])
dir_name = metadata_handler.dir_name
params = metadata_handler.params_dict
#dir_name = easygui.diropenbox()
os.chdir(dir_name)
file_list = metadata_handler.file_list
hdf5_name = metadata_handler.hdf5_name
# Open the hdf5 file
hf5 = tables.open_file(hdf5_name, 'r+')

# Clean up the memory monitor files, pass if clean up has been done already
if args.taste == 'True':
	if not os.path.exists('./memory_monitor_clustering_taste/memory_usage.txt'):
		file_list = os.listdir('./memory_monitor_clustering_taste')
		f = open('./memory_monitor_clustering_taste/memory_usage.txt', 'w')
		for files in file_list:
			try:
				mem_usage = np.loadtxt('./memory_monitor_clustering_taste/' + files)
				print('electrode'+files[:-4], '\t', str(mem_usage)+'MB', file=f)
				os.system('rm ' + './memory_monitor_clustering_taste/' + files)
			except:
				pass	
		f.close()
else:
	if not os.path.exists('./memory_monitor_clustering/memory_usage.txt'):
		file_list = os.listdir('./memory_monitor_clustering')
		f = open('./memory_monitor_clustering/memory_usage.txt', 'w')
		for files in file_list:
			try:
				mem_usage = np.loadtxt('./memory_monitor_clustering/' + files)
				print('electrode'+files[:-4], '\t', str(mem_usage)+'MB', file=f)
				os.system('rm ' + './memory_monitor_clustering/' + files)
			except:
				pass	
		f.close()

# Delete the raw node, if it exists in the hdf5 file, to cut down on file size
if args.taste == 'True':
	try:
			hf5.remove_node('/raw_taste', recursive = 1)
	except:
			print("Raw taste recordings have already been removed, so moving on ..")
else:
	try:
			hf5.remove_node('/raw', recursive = 1)
			try:
				hf5.remove_node('/sorted_units', recursive = 1)
			except:
				print("No sorted units in original hdf5 file.")
			try:
				hf5.remove_node('/unit_descriptor', recursive = 1)
			except:
				print("No unit descriptor in original hdf5 file.")
			 # And if successful, close the currently open hdf5 file and ptrepack the file
			hf5.close()
			print("Raw recordings removed")
			os.system("ptrepack --chunkshape=auto --propindexes --complevel=9 "
				"--complib=blosc " + hdf5_name + " " + hdf5_name[:-3] + "_repacked.h5")
			# Delete the old (raw and big) hdf5 file
			os.system("rm " + hdf5_name)
			# And open the new, repacked file
			hf5 = tables.open_file(hdf5_name[:-3] + "_repacked.h5", 'r+')
			print("File repacked")
	except:
			print("Raw recordings have already been removed, so moving on ..")
		
# Make the sorted_units group in the hdf5 file if it doesn't already exist
try:
		hf5.create_group('/', 'sorted_units')
except:
		pass

# Define a unit_descriptor class to be used to add things (anything!) 
# about the sorted units to a pytables table
class unit_descriptor(tables.IsDescription):
		unit_number = tables.Int32Col(pos=0)
		electrode_number = tables.Int32Col()
		single_unit = tables.Int32Col()
		regular_spiking = tables.Int32Col()
		fast_spiking = tables.Int32Col()
		waveform_count = tables.Int32Col()
		#mean_amplitude = tables.Float32Col()

# Make a table under /sorted_units describing the sorted units. 
# If unit_descriptor already exists, just open it up in the variable table
try:
		table = hf5.create_table('/', 'unit_descriptor', 
				description = unit_descriptor)
except:
		table = hf5.root.unit_descriptor

# Run an infinite loop as long as the user wants to pick clusters from the electrodes   
counter = len(hf5.root.unit_descriptor) - 1
while True:

		unit_details_bool = 0
		counter += 1
		# If sort_file given, iterate through that, otherwise ask user
		if args.sort_file is not None:
			if counter == len(sort_table):
				break
			electrode_num = int(sort_table.Chan[counter])
			num_clusters = int(sort_table.Solution[counter])
			clusters = re.findall('[0-9]+',str(sort_table.Cluster[counter]))
			clusters = [int(x) for x in clusters]

		else:
			# Get electrode number from user
			if args.taste == 'True':
				electrode_num_str, continue_bool = entry_checker(\
						msg = 'Electrode number :: ',
						check_func = str.isdigit,
						fail_response = 'Please enter an integer')
			else:
				electrode_num_str, continue_bool = entry_checker(\
						msg = 'Unit number :: ',
						check_func = str.isdigit,
						fail_response = 'Please enter an integer')

			if continue_bool:
				electrode_num = int(electrode_num_str)
			else:
				break

			num_clusters_str, continue_bool = entry_checker(\
					msg = 'Solution number :: ',
					check_func = lambda x: (str.isdigit(x) and (1<int(x)<=max(params['max_clusters'],25))),
					fail_response = 'Please enter an integer')
			if continue_bool:
				num_clusters = int(num_clusters_str)
			else:
				continue


			clusters_msg, continue_bool = entry_checker(\
					msg = 'Cluster numbers (anything separated) ::',
					check_func = cluster_check,
					fail_response = 'Please enter integers')
			if continue_bool:
				clusters = re.findall('[0-9]+',clusters_msg)
				clusters = [int(x) for x in clusters]
			else:
				continue

		
		# Print out selections
		if args.taste == 'True':
			print('||| Electrode {}, Solution {}, Cluster {} |||'.\
				format(electrode_num, num_clusters, clusters))
		else:
			print('||| Unit {}, Solution {}, Cluster {} |||'.\
				format(electrode_num, num_clusters, clusters))


		# Load data from the chosen electrode and solution
		if args.taste == 'True':
			loading_paths = [\
				f'./spike_waveforms_taste/electrode{electrode_num:02}/spike_waveforms.npy',
				f'./spike_times_taste/electrode{electrode_num:02}/spike_times.npy',
				f'./spike_waveforms_taste/electrode{electrode_num:02}/pca_waveforms.npy',
				f'./spike_waveforms_taste/electrode{electrode_num:02}/energy.npy',
				f'./spike_waveforms_taste/electrode{electrode_num:02}/spike_amplitudes.npy',
				f'./clustering_results_taste/electrode{electrode_num:02}/'\
						f'clusters{num_clusters}/predictions.npy',]
		else:
			loading_paths = [\
				f'./spike_waveforms/unit_{electrode_num:02}/spike_waveforms.npy',
				f'./spike_times/unit_{electrode_num:02}/spike_times.npy',
				f'./spike_waveforms/unit_{electrode_num:02}/pca_waveforms.npy',
				f'./spike_waveforms/unit_{electrode_num:02}/energy.npy',
				f'./spike_waveforms/unit_{electrode_num:02}/spike_amplitudes.npy',
				f'./clustering_results/unit_{electrode_num:02}/'\
						f'clusters{num_clusters}/predictions.npy',]

		var_names = ['spike_waveforms','spike_times','pca_slices','energy',\
				'amplitudes','predictions',]

		for var, path in zip(var_names, loading_paths):
			globals()[var] = np.load(path)

		# Re-show images of neurons so dumb people like Abu can make sure they
		# picked the right ones
		#if ast.literal_eval(args.show_plot):
		if args.show_plot == 'False':
			fig, ax = plt.subplots(len(clusters), 2)
			for cluster_num, cluster in enumerate(clusters):
				if args.taste == 'True':
					isi_plot = mpimg.imread(
							'./Plots_taste/{:02}/clusters{}/'\
											'Cluster{}_ISIs.png'\
											.format(electrode_num, num_clusters, cluster)) 
					waveform_plot =  mpimg.imread(
							'./Plots_taste/{:02}/clusters{}/'\
											'Cluster{}_waveforms.png'\
											.format(electrode_num, num_clusters, cluster)) 
				else:
					isi_plot = mpimg.imread(
							'./Plots_taste/{:02}/clusters{}/'\
											'Cluster{}_ISIs.png'\
											.format(electrode_num, num_clusters, cluster)) 
					waveform_plot =  mpimg.imread(
							'./Plots_taste/{:02}/clusters{}/'\
											'Cluster{}_waveforms.png'\
											.format(electrode_num, num_clusters, cluster)) 
				if len(clusters) < 2:
					ax[0].imshow(isi_plot,aspect='auto');ax[0].axis('off')
					ax[1].imshow(waveform_plot,aspect='auto');ax[1].axis('off')
				else:
					ax[cluster_num, 0].imshow(isi_plot,aspect='auto');
					ax[cluster_num,0].axis('off')
					ax[cluster_num, 1].imshow(waveform_plot,aspect='auto');
					ax[cluster_num,1].axis('off')
			fig.suptitle('Are these the neurons you want to select?')
			fig.tight_layout()
			plt.show()

		# Check if the user wants to merge clusters if more than 1 cluster was chosen. 
		# Else ask if the user wants to split/re-cluster the chosen cluster
		merge = False
		re_cluster = False
		merge_msg = 'a'
		re_cluster_msg = 'a'
		if len(clusters) > 1:
			# Providing more than one cluster will AUTOMATICALLY merge
			merge = True

		else:
			# if sort_file present use that
			if args.sort_file is not None:
				split_element = sort_table.Split[counter]
				if not (split_element.strip() == ''):
						re_cluster = True
				else:
						re_cluster = False
			# Otherwise ask user
			else:
				split_msg, continue_bool = entry_checker(\
						msg = 'SPLIT this cluster? (y/n)',
						check_func = lambda x: x in ['y','n'],
						fail_response = 'Please enter (y/n)')
				if continue_bool:
					if split_msg == 'y': 
						re_cluster = True
					elif split_msg == 'n': 
						re_cluster = False
				else:
					continue


		# If the user asked to split/re-cluster, 
		# ask them for the clustering parameters and perform clustering
		split_predictions = []
		chosen_split = 0
		if re_cluster: 
			# Get clustering parameters from user
			n_clusters = int(input('Number of clusters (default=5): ') or "5")
			values = [100,0.001,10]
			fields_str = (
					f':: Max iterations (1000 is plenty) : {values[0]} \n' 
					f':: Convergence criterion (usually 0.0001) : {values[1]} \n' 
					f':: Number of random restarts (10 is plenty) : {values[2]}')
			print(fields_str) 
			edit_bool = 'a'
			edit_bool_msg, continue_bool = entry_checker(\
					msg = 'Use these parameters? (y/n)',
					check_func = lambda x: x in ['y','n'],
					fail_response = 'Please enter (y/n)')
			if continue_bool:
				if edit_bool_msg == 'y':
					n_iter = values[0] 
					thresh = values[1] 
					n_restarts = values[2] 

				elif edit_bool_msg == 'n': 
					clustering_params = easygui.multenterbox(msg = 'Fill in the'\
							'parameters for re-clustering (using a GMM)', 
							fields  = fields, values = values)
					n_iter = int(clustering_params[1])
					thresh = float(clustering_params[2])
					n_restarts = int(clustering_params[3]) 
			else:
				continue

			# Make data array to be put through the GMM - 5 components: 
			# 3 PCs, scaled energy, amplitude
			this_cluster = np.where(predictions == int(clusters[0]))[0]
			n_pc = 3
			data = np.zeros((len(this_cluster), n_pc + 3))  
			data[:,3:] = pca_slices[this_cluster,:n_pc]
			data[:,0] = np.squeeze(energy[this_cluster]/np.max(energy[this_cluster]))
			data[:,1] = np.squeeze(np.abs(amplitudes[this_cluster])/\
					np.max(np.abs(amplitudes[this_cluster])))

			# Cluster the data
			g = GaussianMixture(
					n_components = n_clusters, 
					covariance_type = 'full', 
					tol = thresh, 
					max_iter = n_iter, 
					n_init = n_restarts)
			g.fit(data)
		
			# Show the cluster plots if the solution converged
			if g.converged_:
				split_predictions = g.predict(data)
				x = np.arange(len(spike_waveforms[0])) + 1
				#fig, ax = gen_square_subplots(n_clusters,sharex=True,sharey=True)
				for cluster in range(n_clusters):
					split_points = np.where(split_predictions == cluster)[0]
					# Waveforms and times from the chosen cluster
					slices_dejittered = spike_waveforms[this_cluster, :]			
					times_dejittered = spike_times[this_cluster]
					# Waveforms and times from the chosen split of the chosen cluster
					times_dejittered = times_dejittered[split_points]			   
					ISIs = np.ediff1d(np.sort(times_dejittered))/30.0
					violations1 = 100.0*float(np.sum(ISIs < 1.0)/split_points.shape[0])
					violations2 = 100.0*float(np.sum(ISIs < 2.0)/split_points.shape[0])
					fig, ax = blech_waveforms_datashader.waveforms_datashader(\
							slices_dejittered[split_points, :], x, downsample = False)
					ax.set_xlabel('Sample (30 samples per ms)')
					ax.set_ylabel('Voltage (microvolts)')
					print_str = (f'\nSplit Cluster {cluster} \n'
						f'{violations2:.1f} % (<2ms),'
						f'{violations1:.1f} % (<1ms),'
						f'{split_points.shape[0]} total waveforms. \n') 
					ax.set_title(print_str)
			else:
				print("Solution did not converge "\
						"- try again with higher number of iterations "\
						"or lower convergence criterion")
				continue

			plt.show()

			# Ask the user for the split clusters they want to choose
			choice_list = tuple([str(i) for i in range(n_clusters)]) 

			chosen_msg, continue_bool = entry_checker(\
					msg = f'Please select from {choice_list} (anything separated) '\
					':: "111" for all ::',
					check_func = cluster_check,
					fail_response = 'Please enter integers')
			if continue_bool:
				chosen_clusters = re.findall('[0-9]+|-[0-9]+',chosen_msg)
				chosen_split = [int(x) for x in chosen_clusters]
				negative_vals = re.findall('-[0-9]+',chosen_msg)
				# If 111, select all
				if 111 in chosen_split:
					chosen_split = range(n_clusters)
					# If any are negative, go into removal mode
				elif len(negative_vals) > 0:
					remove_these = [abs(int(x)) for x in negative_vals]
					chosen_split = [x for x in range(n_clusters) \
							if x not in remove_these]
				print(f'Chosen splits {chosen_split}')
			else:
				continue

		##################################################

		# Get list of existing nodes/groups under /sorted_units
		node_list = hf5.list_nodes('/sorted_units')

		# if sort_table given, use that to name units
		if args.sort_file is not None:
			unit_name = 'unit%03d' % int(true_index[counter])

		else:
			# If node_list is empty, start naming units from 000
			unit_name = ''
			max_unit = 0
			if node_list == []:			 
					unit_name = 'unit%03d' % 0
			# Else name the new unit by incrementing the last unit by 1 
			else:
					unit_numbers = []
					for node in node_list:
							unit_numbers.append(node._v_pathname.split('/')[-1][-3:])
							unit_numbers[-1] = int(unit_numbers[-1])
					unit_numbers = np.array(unit_numbers)
					max_unit = np.max(unit_numbers)
					unit_name = 'unit%03d' % int(max_unit + 1)

		# Get a new unit_descriptor table row for this new unit
		unit_description = table.row	
		# Put in unit number
		if args.sort_file is not None:
			unit_description['unit_number'] = int(true_index[counter])
		else:
			unit_description['unit_number'] = int(max_unit + 1)


		# If the user re-clustered/split clusters, 
		# add the chosen clusters in split_clusters
		if re_cluster:
				hf5.create_group('/sorted_units', unit_name)
				# Waveforms of originally chosen cluster
				cluster_inds = np.where(predictions == int(clusters[0]))[0] 
				fin_inds = np.concatenate(\
						[np.where(split_predictions == this_split)[0] \
									for this_split in chosen_split])
				unit_waveforms = spike_waveforms[cluster_inds, :]	
				# Subsetting this set of waveforms to include only the chosen split
				unit_waveforms = unit_waveforms[fin_inds]

				# Do the same thing for the spike times
				unit_times = spike_times[cluster_inds]
				unit_times = unit_times[fin_inds] 
				# Add to HDF5
				waveforms = hf5.create_array('/sorted_units/%s' % unit_name, 
								'waveforms', unit_waveforms)
				times = hf5.create_array('/sorted_units/%s' % unit_name, \
											'times', unit_times)
				unit_description['waveform_count'] = int(len(unit_times))
				unit_description['electrode_number'] = electrode_num

				# To consolidate asking for unit details (single unit vs multi,
				# regular vs fast), set bool and ask for details at the end
				unit_details_bool = 1
				unit_details_file_bool = 0
				
		##################################################

		# If only 1 cluster was chosen (and it wasn't split), 
		# add that as a new unit in /sorted_units. 
		# Ask if the isolated unit is an almost-SURE single unit
		elif len(clusters) == 1:
				hf5.create_group('/sorted_units', unit_name)
				fin_inds = np.where(predictions == int(clusters[0]))[0]
				unit_waveforms = spike_waveforms[fin_inds, :]
				unit_times = spike_times[fin_inds]
				waveforms = hf5.create_array('/sorted_units/%s' % unit_name, 
						'waveforms', unit_waveforms)
				times = hf5.create_array('/sorted_units/%s' % unit_name, \
										'times', unit_times)
				unit_description['waveform_count'] = int(len(unit_times))
				unit_description['electrode_number'] = electrode_num

				# To consolidate asking for unit details (single unit vs multi,
				# regular vs fast), set bool and ask for details at the end
				unit_details_bool = 1
				# If unit was not manipulated (merge/split), read unit details
				# from file if provided
				unit_details_file_bool = 1

		else:
			# If the chosen units are going to be merged, merge them
			if merge:
				fin_inds = np.concatenate(\
						[np.where(predictions == int(cluster))[0] \
						for cluster in clusters])
				unit_waveforms = spike_waveforms[fin_inds, :]
				unit_times = spike_times[fin_inds]

				# Show the merged cluster to the user, 
				# and ask if they still want to merge
				x = np.arange(len(unit_waveforms[0])) + 1
				fig, ax = blech_waveforms_datashader.\
						waveforms_datashader(unit_waveforms, x, downsample = False)
				# plt.plot(x - 15, unit_waveforms[:, ::10].T, 
				#				   linewidth = 0.01, color = 'red')
				ax.set_xlabel('Sample (30 samples per ms)')
				ax.set_ylabel('Voltage (microvolts)')
				ax.set_title('Merged cluster, No. of waveforms={:d}'.\
									format(unit_waveforms.shape[0]))
				plt.show()
 
				# Warn the user about the frequency of ISI violations 
				# in the merged unit
				ISIs = np.ediff1d(np.sort(unit_times))/30.0
				violations1 = 100.0*float(np.sum(ISIs < 1.0)/len(unit_times))
				violations2 = 100.0*float(np.sum(ISIs < 2.0)/len(unit_times))
				print_str = (f':: Merged cluster \n'
					f':: {violations1:.1f} % (<2ms)\n'
					f':: {violations2:.1f} % (<1ms)\n'
					f':: {len(unit_times)} Total Waveforms \n' 
					':: I want to still merge these clusters into one unit (y/n) :: ')
				proceed_msg, continue_bool = entry_checker(\
						msg = print_str, 
						check_func = lambda x: x in ['y','n'],
						fail_response = 'Please enter (y/n)')
				if continue_bool:
					if proceed_msg == 'y': 
						proceed = True
					elif proceed_msg == 'n': 
						proceed = False
				else:
					continue

				# Create unit if the user agrees to proceed, 
				# else abort and go back to start of the loop 
				if proceed:	 
					hf5.create_group('/sorted_units', unit_name)
					waveforms = hf5.create_array('/sorted_units/%s' % unit_name, 
							'waveforms', unit_waveforms)
					times = hf5.create_array('/sorted_units/%s' % unit_name, 
							'times', unit_times)
					unit_description['waveform_count'] = int(len(unit_times))
					unit_description['electrode_number'] = electrode_num

					# To consolidate asking for unit details (single unit vs multi,
					# regular vs fast), set bool and ask for details at the end
					unit_details_bool = 1
					unit_details_file_bool = 0

				else:
					continue

		# Ask user for unit details, and ask for HDF5 file 
		if unit_details_bool:
			unit_description['regular_spiking'] = 0
			unit_description['fast_spiking'] = 0

			if unit_details_file_bool and (args.sort_file is not None):
				single_unit_msg = sort_table.single_unit[counter]
				if not (single_unit_msg.strip() == ''):
					single_unit = True

					# If single unit, check unit type
					unit_type_msg = sort_table.Type[counter] 
					if unit_type_msg == 'r': 
						unit_type = 'regular_spiking'
					elif unit_type_msg == 'f': 
						unit_type = 'fast_spiking'
					unit_description[unit_type] = 1
				else:
					single_unit = False

			else:
				single_unit_msg, continue_bool = entry_checker(\
						msg = 'Single-unit? (y/n)',
						check_func = lambda x: x in ['y','n'],
						fail_response = 'Please enter (y/n)')
				if continue_bool:
					if single_unit_msg == 'y': 
						single_unit = True
					elif single_unit_msg == 'n': 
						single_unit = False
				else:
					continue


				# If the user says that this is a single unit, 
				# ask them whether its regular or fast spiking
				if single_unit:
					unit_type_msg, continue_bool = entry_checker(\
							msg = 'Regular or fast spiking? (r/f)',
							check_func = lambda x: x in ['r','f'],
							fail_response = 'Please enter (r/f)')
					if continue_bool:
						if unit_type_msg == 'r': 
							unit_type = 'regular_spiking'
						elif unit_type_msg == 'f': 
							unit_type = 'fast_spiking'
					else:
						continue
					unit_description[unit_type] = 1

			unit_description['single_unit'] = int(single_unit)

			unit_description.append()
			table.flush()
			hf5.flush()

		#is_nrn_path = f'./clustering_results/electrode{electrode_num:02}/'\
		#			f'clusters{num_clusters}/is_nrn.npy'
		#if not os.path.exists(is_nrn_path):
		#	is_nrn_list = [[fin_inds]]
		#else:
		#	is_nrn_list = np.load(is_nrn_path)
		#	is_nrn_list.append([fin_inds])
		#np.save(is_nrn_path, is_nrn_list)

		print('==== {} Complete ===\n'.format(unit_name))
		print('==== Iteration Ended ===\n')

# Sort unit_descriptor by unit_number
# This will be needed if sort_table is used, as using sort_table
# will add merge/split marked units first
temp_table = table[:]
temp_table.sort(order = 'unit_number')
table[:] = temp_table
table.flush()
hf5.flush()

print('== Post-processing exiting ==')
print(f'== {len(hf5.root.unit_descriptor)} total units')
if len(hf5.root.unit_descriptor) == hf5.root.sorted_units._g_getnchildren():
	print('== unit_descriptor and sorted units counts match ==')
	# If things match, renumber sorted_units and unit_num in unit_descriptor
	# to be continuously increasing (in case units from the sort-table
	# were deleted or not selected
	# This should not be a problem if units were enterred manually
	if args.sort_file is not None:
		# Since we're not indexing by name, we can rename directly
		base_dir = '/sorted_units'
		node_list = hf5.list_nodes(base_dir)
		final_nums = np.arange(len(node_list))
		for this_node,inter_num in zip(node_list,final_nums):
			this_node._f_rename('unit{:03d}'.format(inter_num))
		# Also rename unit_nums in unit_descriptor
		temp_table = table[:]
		for unit, this_unit_num in zip(temp_table, final_nums):
			unit[0] = this_unit_num
		table[:] = temp_table
else:
	print('== unit_descriptor and sorted units counts **DO NOT** match ==')
# Close the hdf5 file
hf5.close()
